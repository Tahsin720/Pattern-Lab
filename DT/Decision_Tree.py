# -*- coding: utf-8 -*-
"""Assignment_2(DT).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12v64fQ7j_lS-QjlbJngPqsBOW4Ff7ETP
"""

import numpy as np
import math
import random
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Assignment_2(DT)/WA_Fn-UseC_-Telco-Customer-Churn.csv',
                   na_values=["No internet service", "No phone service"])
df.dropna()
df.dropna(inplace=True)
df.reset_index(drop=True)

df.drop(labels = ["SeniorCitizen", "tenure", "MonthlyCharges", "TotalCharges"], axis = 1, inplace = True)

df = df.replace({
    "Churn": {
        "Yes": 1,
        "No": 0
    }
})
label_class = df["Churn"]
df = np.array(df)
print(label_class)

X_train, y_test = train_test_split(df, train_size = 0.8, random_state = 911)
X_train_st, y_test_st = train_test_split(df, train_size = 0.8, stratify = label_class,random_state = 911)

print(y_test[:, y_test.shape[1] - 1])

class Node:
    def __init__(self, attribute=None, attribute_values=None, child_nodes=None, decision=None):
        self.attribute = attribute
        self.attribute_values = attribute_values
        self.child_nodes = child_nodes
        self.decision = decision


class DecisionTree:

    root = None

    @staticmethod
    def plurality_values(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        ### TODO
        label_count_1 = 0
        label_count_0 = 0
        for i in labels:
          if (i == 1):
            label_count_1 = label_count_1 + 1
          else:
            label_count_0 = label_count_0 + 1
          
          if (label_count_1 > label_count_0):
            return True
          else:
            return False

    @staticmethod
    def all_zero(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        ### TODO

        for x in labels:
            if x != 0:
                return False
        return True


    @staticmethod
    def all_one(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        ### TODO
        
        for x in labels:
            if x != 1:
                return False
        return True

    @staticmethod
    def importance(data, attributes):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        ### TODO
        labels = np.array(labels)
        class_type = len(set(labels))
        count_labels = {}

        for x in labels:
            if x not in count_labels:
                count_labels[x] = 0
            count_labels[x] += 1
        
        curr_entropy = 0
        for i, (k, v) in enumerate(count_labels.items()):
            curr_entropy += (-v / len(labels)) * math.log2(v / len(labels))
        max_gain = 0
        max_gained_feature = 0

        for x in range(data.shape[1]-1):
            if x not in attributes:
                continue
            extracted_features = data[:,x]
            extracted_features_values = set(extracted_features)

            count_info = {}
            for val in extracted_features_values:
                count_info[val] = [0] * class_type
            it = 0
            for val in extracted_features:
                count_info[val][labels[it]] += 1
                it += 1
            curr_gain = curr_entropy
            for val in extracted_features_values:
                current_entropy = 0
                for label in count_info[val]:
                    if label != 0:
                        current_entropy += (-label / sum(count_info[val])) * math.log2(label / sum(count_info[val]))
                current_entropy *= (sum(count_info[val])/len(labels))
                curr_gain -= current_entropy
            
            if curr_gain > max_gain:
                max_gain = curr_gain
                max_gained_feature = x
        
        return max_gained_feature

    def train(self, data, attributes, parent_data):
        data = np.array(data)
        parent_data = np.array(parent_data)
        attributes = list(attributes)

        if data.shape[0] == 0:  # if x is empty
            return Node(decision=self.plurality_values(parent_data))

        elif self.all_zero(data):
            return Node(decision=0)

        elif self.all_one(data):
            return Node(decision=1)

        elif len(attributes) == 0:
            return Node(decision=self.plurality_values(data))

        else:
            a = self.importance(data, attributes)
            tree = Node(attribute=a, attribute_values=np.unique(data[:, a]), child_nodes=[])
            attributes.remove(a)
            for vk in np.unique(data[:, a]):
                new_data = data[data[:, a] == vk, :]
                subtree = self.train(new_data, attributes, data)
                tree.child_nodes.append(subtree)

            return tree

    def fit(self, data):
        self.root = self.train(data, list(range(data.shape[1] - 1)), np.array([]))

    def predict(self, data):
        predictions = []
        for i in range(data.shape[0]):
            current_node = self.root
            while True:
                if current_node.decision is None:
                    current_attribute = current_node.attribute
                    current_attribute_value = data[i, current_attribute]
                    if current_attribute_value not in current_node.attribute_values:
                        predictions.append(random.randint(0, 1))
                        break
                    idx = list(current_node.attribute_values).index(current_attribute_value)

                    current_node = current_node.child_nodes[idx]
                else:
                    predictions.append(current_node.decision)
                    break

        return predictions

######### ===========>>>>>>> Function for accuracy and TF, FP, TN, FN <<<<<<<=========== #########
def matrix(prediction, y_test):
  tn = 0 
  fp = 0 
  fn = 0 
  tp = 0
  y_test = np.array(y_test)
  for i in range(len(prediction)):
    if y_test[i] == prediction[i] == 1:
      tp += 1
    elif prediction[i] == 1 and y_test[i] != prediction[i]:
      fp += 1
    elif y_test[i] == y_test[i] == 0:
      tn += 1
    elif prediction[i] == 0 and y_test[i] != prediction[i]:
      fn += 1
  return(tn, fp, fn, tp)

def acc_rate(prediction, test_y):
  test_y = np.array(test_y)
  count = 0
  for i in range(len(prediction)):
    if prediction[i] == test_y[i]:
      count += 1
  # print(count)
  return(count / len(test_y))

######### ===========>>>>>>> Testing the code here with accuracy, precision, recall, f1_sc <<<<<<<=========== #########
######### ===========>>>>>>> Using libray without stratification <<<<<<<=========== #########
DT = DecisionTree()

DT.fit(X_train)

Predict = DT.predict(y_test)

tn, fp, fn, tp = confusion_matrix(list(y_test[:, y_test.shape[1] - 1]), Predict).ravel()
print(tn, fp, fn, tp)

accuracy = accuracy_score(list(y_test[:, y_test.shape[1] - 1]), Predict)
precision = precision_score(list(y_test[:, y_test.shape[1] - 1]), Predict)
recall = recall_score(list(y_test[:, y_test.shape[1] - 1]), Predict)
f1_sc = f1_score(list(y_test[:, y_test.shape[1] - 1]), Predict)

print("Accuracy: " , accuracy, " >>>> " , "Precision: ", precision, " >>>> " , "Recall: ", recall, " >>>> " , "f1_score: " , f1_sc)

######### ===========>>>>>>> Using libray with stratification <<<<<<<=========== #########

DT1 = DecisionTree()

DT1.fit(X_train_st)

Predict1 = DT.predict(y_test_st)

tn, fp, fn, tp = confusion_matrix(list(y_test_st[:, y_test_st.shape[1] - 1]), Predict1).ravel()
print(tn, fp, fn, tp)

accuracy = accuracy_score(list(y_test_st[:, y_test_st.shape[1] - 1]), Predict1)
precision = precision_score(list(y_test_st[:, y_test.shape[1] - 1]), Predict1)
recall = recall_score(list(y_test_st[:, y_test_st.shape[1] - 1]), Predict1)
f1_sc = f1_score(list(y_test_st[:, y_test_st.shape[1] - 1]), Predict1)

print("Accuracy: " , accuracy, " >>>> " , "Precision: ", precision, " >>>> " , "Recall: ", recall, " >>>> " , "f1_score: " , f1_sc)

######### ===========>>>>>>> Without using libray and stratification <<<<<<<=========== #########

accuracy = acc_rate(Predict, y_test[:, y_test.shape[1] - 1])

tn, fp, fn, tp = matrix(Predict, y_test[:, y_test.shape[1] - 1])
print(tn, fp, fn, tp)

precision = (tp / (tp + fp))
recall = (tp / (tp + fn))
f1_sc = 2 * ((precision * recall) / (precision + recall))

print("Accuracy: " , accuracy , " " , "Precision: ", precision, "Recall: ", recall, " " , "f1_score: " , f1_sc)

######### ===========>>>>>>> Without using libray and with stratification <<<<<<<=========== #########

accuracy = acc_rate(Predict1, y_test_st[:, y_test_st.shape[1] - 1])

tn, fp, fn, tp = matrix(Predict1, y_test_st[:, y_test_st.shape[1] - 1])
print(tn, fp, fn, tp)

precision = (tp / (tp + fp))
recall = (tp / (tp + fn))
f1_sc = 2 * ((precision * recall) / (precision + recall))

print("Accuracy: " , accuracy , " " , "Precision: ", precision, "Recall: ", recall, " " , "f1_score: " , f1_sc)