# -*- coding: utf-8 -*-
"""Group_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pgqrf7yhcechH9S-ACkBBefVbWKj1aRW

# GROUP 1
## *Vulgarity Detection in a sentence*


---



## Members:

| Name | ID |
|--|--|
| Rokibul Islam | 011181102 |
|Tahsin Habib Brinto |011181107|
|Chion Ghosh |011181110|
|Hasibul Hasan |011181117|



---



> CSE 416 (A) // Project
"""

import re
import string

import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import time

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

from google.colab import drive
drive.mount('/content/drive')

text = []
labels = []
with open('/content/drive/MyDrive/Colab Notebooks/pattern_project/_warner.txt', 'r') as f:
  with open('/content/drive/MyDrive/Colab Notebooks/pattern_project/data.csv', 'w', newline= '') as data:
    d = f.readlines()
    instances = [(d[i],d[i+1]) for i in range(0,len(d),3)]
    for instance in instances:
      label_class = instance[0]
      feature = instance[1]

      if 'notAbusive' in label_class:
        label = 0
      else:
        label = 1

      text.append(feature.replace('\n', ''))
      labels.append(label)

    df = pd.DataFrame(list(zip(text, labels)), columns=['text', 'target'])

df.head()

"""#Text Cleaning and Preprocessing"""

stop = set(stopwords.words("english"))

def remove_stopwords(text):
    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]
    return " ".join(filtered_words)

def remove_URL(text):
    url = re.compile(r"https?://\S+|www\.\S+")
    return url.sub(r"", text)

def remove_punct(text):
    translator = str.maketrans("", "", string.punctuation)
    return text.translate(translator)
  
def remove_emoji(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                            "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

df["text"] = df.text.map(remove_stopwords)
df["text"] = df.text.map(remove_URL)
df["text"] = df.text.map(remove_punct)
df["text"] = df.text.map(remove_emoji)

from collections import Counter

# Count unique words
def counter_word(text_col):
    count = Counter()
    for text in text_col.values:
        for word in text.split():
            count[word] += 1
    return count

counter = counter_word(df.text)

num_unique_words = len(counter)

"""#Over sampling our data as it's imbalanced
### Using SMOTE (Synthetic Minority Over-sampling Technique)
"""

from sklearn.model_selection import train_test_split
X = df.text
y = df.target

train_sentences, val_sentences, train_labels, val_labels = train_test_split(X, y, train_size=0.8, random_state=911, stratify=y)

from imblearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from imblearn.over_sampling import SMOTE


# textclassifier =Pipeline([
#   ('vect', CountVectorizer()),
#    ('tfidf', TfidfTransformer()),
#    ('smote', SMOTE(random_state=12)),
#    ('mnb', MultinomialNB(alpha =0.1))
# ])

# textclassifier.fit(train_sentences, train_labels)

"""# Tokenizing text"""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=num_unique_words, oov_token='<OOV>')
tokenizer.fit_on_texts(train_sentences) # fitting only to train data

# each word has unique index
word_index = tokenizer.word_index

"""# Test data (Manual)"""

test_sentances = [
    "@IAmThe1Teacher You can preach coexistence all you want.  To Islamist you are just a useful idiot.",
    "Once I'm done with my work, I'll have walk ouside",
    "Your are a noob and fucking moron.",
    "This game is fucking addictive and I love it!",
    "Roses are red, violates are blue. She likes everybody but you",
    "Kat may be a bitch but let's be honest, the blondes weren't too far behind",
    "Drasko they didn't cook half a bird you idiot"
]

test_labels = [1, 0, 1, 0, 0, 1]

"""## Generating sequesnces for the train and validation data"""

train_sequences = tokenizer.texts_to_sequences(train_sentences)
val_sequences = tokenizer.texts_to_sequences(val_sentences)
test_sequences = tokenizer.texts_to_sequences(test_sentances)

"""#Padding
```tf.keras.layers.TextVectorization``` encoder could have been used. It does all the steps including preprocessing, vocabulary sequence --> padding and embedding
"""

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Max number of words in a sequence
max_length = 40

train_padded = pad_sequences(train_sequences, maxlen=max_length, padding="post", truncating="post")
val_padded = pad_sequences(val_sequences, maxlen=max_length, padding="post", truncating="post")
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding="post", truncating="post")
train_padded.shape, val_padded.shape, test_padded.shape

"""#Creating LSTM Model"""

from tensorflow.keras import layers

model = keras.models.Sequential()
model.add(layers.Embedding(num_unique_words, 32 , input_length=max_length))

model.add(layers.LSTM(64, dropout=0.1))
model.add(layers.Dense(1, activation="sigmoid"))
model.summary()

from tensorflow.keras.utils import plot_model
from tensorflow.keras import metrics

model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics = ['Recall'])
              
plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)

hist = model.fit(train_padded, train_labels, epochs=200, validation_data=(val_padded, val_labels), verbose=2)

import matplotlib.pyplot as plt

hist.history
plt.plot(hist.history['recall'])
plt.title('model performance(recall)')
plt.ylabel('Recall')
plt.xlabel('Epoch')
plt.legend(['train'], loc='upper left')
plt.show()

predictions = model.predict(val_padded)
predictions = [1 if p > 0.5 else 0 for p in predictions]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(val_labels, predictions)
recall = recall_score(val_labels, predictions)
precision = precision_score(val_labels, predictions)
f1 = f1_score(val_labels, predictions)

print(f'Accuracy: {accuracy}')
print(f'F1_Score: {f1}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')

"""# References and Doccumentations for later use

 1. https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome
 2. https://www.tensorflow.org/text/tutorials/text_classification_rnn
 3. https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S
 4. https://www.youtube.com/watch?v=aircAruvnKk
 5. https://github.com/python-engineer/tensorflow-course
 6. https://keras.io/api/layers/recurrent_layers/bidirectional/
 7. https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/
 8. https://realpython.com/python-keras-text-classification/
"""